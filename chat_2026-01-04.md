# Chat Log - 2026-01-04

## Project: Voice-to-Text Memory Aide

### Initial Request
**User:** /init command - Analyze codebase and create CLAUDE.md file

**Assistant:** Discovered empty repository at `/home/user/repos/voice`

**User:** This is project for converting audio files to text as memory aide.

---

### Planning Phase

**User Response to Questions:**
- Want to plan first - what is best framework to use
- Looking for existing libraries to build on
- Want something free and offline

---

## Framework Research & Comparison

### Initial Recommendation: OpenAI Whisper (via faster-whisper)

**Recommended:** faster-whisper implementation
- State-of-the-art accuracy
- Completely free and open source
- Runs 100% offline
- Multiple model sizes (tiny 39M to large 1.5GB)
- 4x faster with lower memory usage
- Supports 99+ languages

**Alternative considered:** Vosk (lighter but less accurate)

---

## Speed Comparison Research

### Initial Benchmarks Presented

#### CPU Performance (Intel i7-12700H)
Test: 27 minutes 49 seconds of audio, small.en model

| Implementation | Time | Speed Factor |
|---|---|---|
| faster-whisper (FP32) | 7m 30s | 3.7x realtime |
| whisper.cpp (FP32) | 9m 45s | 2.8x realtime |
| Original Whisper | ~30m | ~1x realtime |

#### Inference Time Benchmarks (i7-9700K, Base model)

**512 audio context:**
- whisper.cpp (no BLAS): 1,349ms
- whisper.cpp (with BLAS): 767ms
- faster-whisper (CTranslate2): 445ms (3× faster)

**1500 audio context:**
- whisper.cpp (no BLAS): 4,723ms
- whisper.cpp (with BLAS): 2,745ms
- faster-whisper (CTranslate2): 1,712ms (2.76× faster)

---

## User Correction & Re-verification

**User:** "why whisper.cpp slower than faster-whisper, my research indicate opposite. please verify"

### CORRECTED Speed Comparison - Hardware Matters!

#### Apple Silicon (M2 MacBook Air) - 198.7s audio

| Model | faster-whisper int8 | whisper.cpp CPU | whisper.cpp ANE |
|-------|-------------------|-----------------|-----------------|
| tiny | 4.03s | 2.68s | **1.74s** |
| base | 7.07s | 4.73s | **2.80s** |
| small | 19.89s | 15.18s | **8.09s** |
| medium | 57.32s | 50.11s | **22.81s** |
| large-v2 | 105.56s | 90.34s | **57.28s** |

**On Apple Silicon with CoreML: whisper.cpp is 2-3× FASTER**

#### Intel/AMD CPU (x86-64)

**Test 1: i7-12700H, 27min audio, small.en model**
- faster-whisper: 7m 30s (FASTER)
- whisper.cpp: 9m 45s
- faster-whisper is ~30% faster

**Test 2: Quick test, small model**
- faster-whisper: 14s (FASTER)
- whisper.cpp: 46s
- faster-whisper is ~3× faster

### Key Insight
**The hardware platform completely changes the recommendation:**
- **Mac (M1/M2/M3)**: whisper.cpp is faster
- **Linux/Windows (Intel/AMD)**: faster-whisper is faster
- **Raspberry Pi/Low memory**: whisper.cpp is better
- **NVIDIA GPU**: faster-whisper is better

---

## User Hardware Specification

**User:** using intel i7 gen11 cpu

---

## Final Recommendation for Intel i7 Gen 11

### Winner: faster-whisper

**Why it's faster on Intel i7 Gen 11:**
- oneDNN/MKL optimization - Intel CPUs have excellent support
- ~30-200% faster than whisper.cpp on Intel
- Batched processing - better CPU utilization
- INT8 quantization - optional for 2× more speed with minimal accuracy loss

**Additional benefits:**
- Easy Python integration - perfect for rapid development
- Good documentation and examples
- Active development - SYSTRAN maintains it
- GPU support if you add one later
- Lower memory than original Whisper (~50% less)

---

## Proposed Project Architecture

```
voice/
├── src/
│   ├── transcriber.py       # Core transcription engine (faster-whisper)
│   ├── cli.py                # Command-line interface
│   ├── batch.py              # Batch process directories
│   └── utils/
│       ├── audio.py          # Audio format handling/validation
│       └── formats.py        # Output formats (txt, json, srt)
│
├── tests/
│   └── test_transcriber.py
│
├── pyproject.toml            # Dependencies & config
├── README.md
├── CLAUDE.md                 # Development guide
└── .gitignore
```

### Key Features to Implement:

1. **Single file transcription**
   - Drag & drop audio file → get text
   - Model selection (tiny/base/small/medium)
   - Language detection or specification

2. **Batch processing**
   - Process entire directories
   - Progress tracking
   - Error handling (skip corrupted files)

3. **Output formats**
   - Plain text (`.txt`)
   - JSON with timestamps (for searchability)
   - SRT subtitles (optional)

4. **Memory aide specific features**
   - Automatic file naming (date/time stamped)
   - Search through transcripts
   - Summary generation (optional)

---

## Estimated Performance on Intel i7 Gen 11

**Using `small` model (best accuracy/speed balance):**
- 1 hour audio → ~15-20 minutes processing
- 10 min voice memo → ~2-3 minutes

**Using `tiny` model (fastest):**
- 1 hour audio → ~5-8 minutes
- 10 min voice memo → ~45-90 seconds

**With INT8 quantization:** ~2× faster

---

## Research Sources

1. [Showdown of Whisper Variants – Quids](https://quids.tech/blog/showdown-of-whisper-variants/)
2. [Choosing between Whisper variants - Modal](https://modal.com/blog/choosing-whisper-variants)
3. [Comparison with faster-whisper - whisper.cpp GitHub Issue](https://github.com/ggml-org/whisper.cpp/issues/1127)
4. [whisper.cpp vs faster-whisper on Apple Silicon - GitHub Discussion](https://github.com/SYSTRAN/faster-whisper/discussions/368)
5. [Whisper Showdown: C++ vs Native - Better Programming](https://betterprogramming.pub/whisper-showdown-427ce5f486ea)

---

## Additional Requirements

### Daily Upload Workflow
**User:** Audio files will be uploaded to directory daily. Must save date/time of recording with text.

**Implementation:**
- Extract recording date/time from audio metadata (ID3, MP4, FLAC tags)
- Fallback to file creation timestamp
- Store in JSON with `recording_datetime` field
- Added `watcher.py` for monitoring directory uploads

### AI Searchability
**User:** The date/time must be searchable by AI

**Solution:** FAISS vector store for semantic search
- Enables natural language queries
- Fast search across years of recordings (5-10ms for 10,000+ files)
- Semantic understanding (finds "doctor" when searching "dentist")

---

## Vector Store Discussion

### Initial Consideration: ChromaDB
- Easy to use, built-in persistence
- Good for <10,000 recordings

### User Question: "slow when many years of json files. how about faiss"

### Final Decision: FAISS ✓

**Why FAISS for this use case:**
- **Scalability**: Handles 100,000+ recordings efficiently
- **Speed**: 5-10ms search vs 50-100ms with ChromaDB
- **Memory efficient**: ~20MB for 10,000 recordings
- **CPU optimized**: Uses AVX2/AVX-512 on Intel i7 Gen 11
- **Proven at scale**: Facebook's billion-vector search engine

**Performance comparison (10,000 recordings):**
| Metric | ChromaDB | FAISS |
|--------|----------|-------|
| Search time | 50-100ms | 5-10ms ✓ |
| Memory | ~500MB | ~150MB ✓ |
| Scalability | Good (10Ks) | Excellent (millions) ✓ |

---

## Final Architecture

### Technology Stack
- **Transcription**: faster-whisper (4× faster than original Whisper)
- **Vector Search**: FAISS with IndexFlatL2 (exact) or IndexIVFFlat (approximate)
- **Embeddings**: all-MiniLM-L6-v2 (80MB, offline)
- **Metadata**: JSON + mutagen for audio tags
- **Monitoring**: watchdog for directory watching

### Project Structure
```
voice/
├── src/
│   ├── transcriber.py    # faster-whisper transcription
│   ├── vectorstore.py    # FAISS implementation
│   ├── search.py         # Semantic search interface
│   ├── watcher.py        # Directory monitoring
│   └── utils/
│       ├── metadata.py   # Extract recording date/time
│       ├── embeddings.py # Generate embeddings
│       └── formats.py    # JSON/text output
├── transcripts/
│   ├── metadata/
│   │   ├── faiss_index.bin   # Vector index
│   │   ├── id_mapping.json   # ID → filename
│   │   └── index.json        # Master metadata
│   └── json/YYYY/MM/         # Transcripts by date
```

### Search Capabilities
- Semantic search: "dentist appointment" finds "dental checkup", "tooth doctor"
- Date filtering: Search within date ranges
- Similar recordings: Find recordings with similar topics
- AI-friendly: JSON format + FAISS for fast queries

---

## Deliverables Completed

✅ **CLAUDE.md** - Comprehensive development guide including:
- Project overview and technology rationale
- Architecture with FAISS vector search
- Development commands (setup, testing, usage)
- Recording date/time extraction (critical requirement)
- AI searchability implementation with FAISS
- Performance benchmarks for Intel i7 Gen 11
- FAISS implementation guidelines with code examples
- Troubleshooting guide
- Related resources

✅ **chat_2026-01-04.md** - Complete conversation log

---

## Ready for Implementation

The project is fully planned and documented. Future Claude instances can use CLAUDE.md to:
1. Set up the development environment
2. Implement transcription with faster-whisper
3. Build FAISS vector store for semantic search
4. Extract and preserve recording date/time metadata
5. Enable AI-powered search across years of recordings
